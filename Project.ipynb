{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# COVID-19 cases and vaccination at São Paulo state - Brazil\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project has the main purpose of provide data about COVID-19 cases, vaccination and hospital overwhelmed at São Paulo state in Brazil.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1 - Scope the Project and Gather Data\n",
    "\n",
    "In this project we are going to create an ETL process for ingest data into a Data Warehouse for an analytical team gets insights about it.\n",
    "\n",
    "Firstly, we get the data from three different sources. The data about COVID-19 cases are available at [Brasil.io API](https://brasil.io/home/), to use this data, to this demonstration, I got the data from this API and download it in json format. In this [link](https://github.com/turicas/covid19-br) you can check more details about how to use Brasil.io API.\n",
    "\n",
    "The [data about hospital overwhelmed](https://opendatasus.saude.gov.br/tl/dataset/registro-de-ocupacao-hospitalar) are available in CSV format, at Ministry of Health (Brazil).\n",
    "\n",
    "At the same way, the [data about vaccination](https://opendatasus.saude.gov.br/tl/dataset/covid-19-vacinacao) at São Paulo state are avilable at Ministry of Health (Brazil) too, in CSV format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, from_unixtime\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import TimestampType, DateType, StructType, StringType, IntegerType, DoubleType, StructField, BooleanType\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import monotonically_increasing_id,row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.5\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Hospital overwhelmed\n",
    "Firstly, we are going to analyze the data about hospital overwhelmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "hospital_path = './covid19_data/leitos_BR.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_hospital = spark.read.option('delimiter', ';').option('header', 'true').csv(hospital_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- dataNotificacao: string (nullable = true)\n",
      " |-- cnes: string (nullable = true)\n",
      " |-- ocupacaoSuspeitoCli: string (nullable = true)\n",
      " |-- ocupacaoSuspeitoUti: string (nullable = true)\n",
      " |-- ocupacaoConfirmadoCli: string (nullable = true)\n",
      " |-- ocupacaoConfirmadoUti: string (nullable = true)\n",
      " |-- saidaSuspeitaObitos: string (nullable = true)\n",
      " |-- saidaSuspeitaAltas: string (nullable = true)\n",
      " |-- saidaConfirmadaObitos: string (nullable = true)\n",
      " |-- saidaConfirmadaAltas: string (nullable = true)\n",
      " |-- origem: string (nullable = true)\n",
      " |-- _p_usuario: string (nullable = true)\n",
      " |-- estadoNotificacao: string (nullable = true)\n",
      " |-- municipioNotificacao: string (nullable = true)\n",
      " |-- estado: string (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      " |-- excluido: string (nullable = true)\n",
      " |-- validado: string (nullable = true)\n",
      " |-- _created_at: string (nullable = true)\n",
      " |-- _updated_at: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_hospital.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Let's check what states are in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|estado             |count |\n",
      "+-------------------+------+\n",
      "|null               |2     |\n",
      "|Acre               |420   |\n",
      "|Alagoas            |3762  |\n",
      "|Amapá              |301   |\n",
      "|Amazonas           |8543  |\n",
      "|Bahia              |14149 |\n",
      "|Ceará              |14023 |\n",
      "|Distrito Federal   |2719  |\n",
      "|Espírito Santo     |5067  |\n",
      "|GOIAS              |1431  |\n",
      "|Goiás              |39860 |\n",
      "|Maranhão           |8493  |\n",
      "|Mato Grosso        |15979 |\n",
      "|Mato Grosso do Sul |17835 |\n",
      "|Minas Gerais       |74280 |\n",
      "|Paraná             |23594 |\n",
      "|Paraíba            |3152  |\n",
      "|Pará               |7031  |\n",
      "|Pernambuco         |26504 |\n",
      "|Piauí              |7070  |\n",
      "|Rio Grande do Norte|7383  |\n",
      "|Rio Grande do Sul  |53968 |\n",
      "|Rio de Janeiro     |27301 |\n",
      "|Rondônia           |3684  |\n",
      "|Roraima            |419   |\n",
      "|Santa Catarina     |17809 |\n",
      "|Sergipe            |3065  |\n",
      "|São Paulo          |104851|\n",
      "|Tocantins          |4832  |\n",
      "+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_hospital.select(['estado']).groupby('estado').count().sort('estado').show(50, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The Ministry of Health (Brazil) provides data about all brazilian states. For this project we going to study only São Paulo, so, we need to filter this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_hospital = df_hospital.filter(df_hospital.estado == 'São Paulo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Secondly, let's check the cities of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|municipio|\n",
      "+---------+\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_hospital.select(['municipio']).where((col('municipio').isNull()) | (col('municipio') == '')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "There aren't data without cities name, so, we don't need work in it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "So, let's check the notification data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     dataNotificacao|\n",
      "+--------------------+\n",
      "|1983-08-15T03:00:...|\n",
      "|1989-08-08T03:00:...|\n",
      "|2010-10-03T03:00:...|\n",
      "|2010-11-24T02:00:...|\n",
      "|2011-08-14T03:00:...|\n",
      "|2019-06-29T03:00:...|\n",
      "|2020-02-01T03:00:...|\n",
      "|2020-02-07T03:00:...|\n",
      "|2020-02-08T03:00:...|\n",
      "|2020-02-19T03:00:...|\n",
      "|2020-03-02T03:00:...|\n",
      "|2020-03-04T03:00:...|\n",
      "|2020-03-12T03:00:...|\n",
      "|2020-03-17T03:00:...|\n",
      "|2020-03-18T03:00:...|\n",
      "|2020-03-19T03:00:...|\n",
      "|2020-03-20T03:00:...|\n",
      "|2020-03-20T03:00:...|\n",
      "|2020-03-21T03:00:...|\n",
      "|2020-03-22T03:00:...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_hospital.select([\n",
    "    'dataNotificacao',\n",
    "]).sort('dataNotificacao').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can see data from 1983. It's doesn't make sense, because we are analyzing hospital overwhelmed as a consequence of COVID-19. Firstly, we are cast the field. Data Notificacao is a string type, for our purpose, we don't need the hours of the date, so, let's cast this field to date type and filter dates from 2020 and 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "get_date = udf(lambda x: datetime.strptime(x[0:10], '%Y-%m-%d'), DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_hospital = df_hospital.withColumn(\"dataNotificacao_date\", get_date(col('dataNotificacao')))\n",
    "df_hospital = df_hospital.filter(df_hospital.dataNotificacao_date > '2020-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|dataNotificacao_date|\n",
      "+--------------------+\n",
      "|          2020-02-01|\n",
      "|          2020-02-07|\n",
      "|          2020-02-08|\n",
      "|          2020-02-19|\n",
      "|          2020-03-02|\n",
      "|          2020-03-04|\n",
      "|          2020-03-12|\n",
      "|          2020-03-17|\n",
      "|          2020-03-18|\n",
      "|          2020-03-19|\n",
      "|          2020-03-20|\n",
      "|          2020-03-20|\n",
      "|          2020-03-21|\n",
      "|          2020-03-22|\n",
      "|          2020-03-23|\n",
      "|          2020-03-23|\n",
      "|          2020-03-24|\n",
      "|          2020-03-24|\n",
      "|          2020-03-25|\n",
      "|          2020-03-25|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_hospital.select([\n",
    "    'dataNotificacao_date',\n",
    "]).sort('dataNotificacao_date').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "For our database, these fields are more sensibles. Let's go to the another database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Vaccination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "vaccine_path = './covid19_data/vacinacao_SP.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_vaccine = spark.read.option('delimiter', ';').option('header', 'true').csv(vaccine_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- document_id: string (nullable = true)\n",
      " |-- paciente_id: string (nullable = true)\n",
      " |-- paciente_idade: string (nullable = true)\n",
      " |-- paciente_dataNascimento: string (nullable = true)\n",
      " |-- paciente_enumSexoBiologico: string (nullable = true)\n",
      " |-- paciente_racaCor_codigo: string (nullable = true)\n",
      " |-- paciente_racaCor_valor: string (nullable = true)\n",
      " |-- paciente_endereco_coIbgeMunicipio: string (nullable = true)\n",
      " |-- paciente_endereco_coPais: string (nullable = true)\n",
      " |-- paciente_endereco_nmMunicipio: string (nullable = true)\n",
      " |-- paciente_endereco_nmPais: string (nullable = true)\n",
      " |-- paciente_endereco_uf: string (nullable = true)\n",
      " |-- paciente_endereco_cep: string (nullable = true)\n",
      " |-- paciente_nacionalidade_enumNacionalidade: string (nullable = true)\n",
      " |-- estabelecimento_valor: string (nullable = true)\n",
      " |-- estabelecimento_razaoSocial: string (nullable = true)\n",
      " |-- estalecimento_noFantasia: string (nullable = true)\n",
      " |-- estabelecimento_municipio_codigo: string (nullable = true)\n",
      " |-- estabelecimento_municipio_nome: string (nullable = true)\n",
      " |-- estabelecimento_uf: string (nullable = true)\n",
      " |-- vacina_grupoAtendimento_codigo: string (nullable = true)\n",
      " |-- vacina_grupoAtendimento_nome: string (nullable = true)\n",
      " |-- vacina_categoria_codigo: string (nullable = true)\n",
      " |-- vacina_categoria_nome: string (nullable = true)\n",
      " |-- vacina_lote: string (nullable = true)\n",
      " |-- vacina_fabricante_nome: string (nullable = true)\n",
      " |-- vacina_fabricante_referencia: string (nullable = true)\n",
      " |-- vacina_dataAplicacao: string (nullable = true)\n",
      " |-- vacina_descricao_dose: string (nullable = true)\n",
      " |-- vacina_codigo: string (nullable = true)\n",
      " |-- vacina_nome: string (nullable = true)\n",
      " |-- sistema_origem: string (nullable = true)\n",
      " |-- data_importacao_rnds: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vaccine.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Firstly, let's check the age of the vaccinated people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|paciente_idade|\n",
      "+--------------+\n",
      "|           221|\n",
      "|           127|\n",
      "|           124|\n",
      "|           123|\n",
      "|           121|\n",
      "|           119|\n",
      "|           117|\n",
      "|           115|\n",
      "|           114|\n",
      "|           113|\n",
      "|           112|\n",
      "|           111|\n",
      "|           110|\n",
      "|           109|\n",
      "|           108|\n",
      "|           107|\n",
      "|           106|\n",
      "|           105|\n",
      "|           104|\n",
      "|           103|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vaccine.selectExpr(\n",
    "    \"cast(paciente_idade as int) paciente_idade\", \n",
    ").dropDuplicates([\n",
    "    'paciente_idade'\n",
    "]).sort('paciente_idade', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As you can see, one people that is 220 years, is so old to receive the vaccine. It's probably an error in our database, we're going to set null for ages above 110 years, because we don't want to lost this information about vaccined people, but we don't know how old they are. For ages less than 18 years, we also set null data, because there aren't vaccine application in children or younger people. We also need convert the ages do Integer type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "cast_age = udf(lambda x: int(x) if int(x) <= 110 or int(x) <= 18 else None, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_vaccine = df_vaccine.withColumn(\"paciente_idade_int\", cast_age(col('paciente_idade')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Let's check if there is a data without application date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|vacina_dataAplicacao|\n",
      "+--------------------+\n",
      "|                null|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vaccine.select(['vacina_dataAplicacao']).where(col('vacina_dataAplicacao').isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Our databases are based on dates. We need to be able related vaccination, hospital overwhelmed and cases and understand the pandemic situation across the days. Because of this, I decided exclude datas without vacination date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_vaccine = df_vaccine.filter(df_vaccine.vacina_dataAplicacao.isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "These field are the most relevant for this dataset, so, we're going to the last dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "cases_path = './covid19_data/api_covid.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_cases = spark.read.json(cases_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- city_ibge_code: string (nullable = true)\n",
      " |-- confirmed: long (nullable = true)\n",
      " |-- confirmed_per_100k_inhabitants: double (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- death_rate: double (nullable = true)\n",
      " |-- deaths: long (nullable = true)\n",
      " |-- estimated_population: long (nullable = true)\n",
      " |-- estimated_population_2019: long (nullable = true)\n",
      " |-- is_last: boolean (nullable = true)\n",
      " |-- order_for_place: long (nullable = true)\n",
      " |-- place_type: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cases.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "When we get the data from the API, we filter the state São Paulo, so, it's not necessary check state field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Firstly, let's check the dates of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2021-03-20|\n",
      "|2021-03-20|\n",
      "|2021-03-20|\n",
      "|2021-03-20|\n",
      "|2021-03-20|\n",
      "|2021-03-20|\n",
      "|2021-03-20|\n",
      "|2021-03-20|\n",
      "|2021-03-20|\n",
      "|2021-03-20|\n",
      "|2021-03-20|\n",
      "|2021-03-20|\n",
      "|2021-03-20|\n",
      "|2021-03-20|\n",
      "|2021-03-20|\n",
      "|2021-03-20|\n",
      "|2021-03-20|\n",
      "|2021-03-20|\n",
      "|2021-03-20|\n",
      "|2021-03-20|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cases.select(['date']).sort('date', ascending=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Our data looks ok, let's check if exists any city without name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|               city|count|\n",
      "+-------------------+-----+\n",
      "|               null|    1|\n",
      "|         Adamantina|    1|\n",
      "|             Adolfo|    1|\n",
      "|              Aguaí|    1|\n",
      "|             Agudos|    1|\n",
      "|           Alambari|    1|\n",
      "|  Alfredo Marcondes|    1|\n",
      "|             Altair|    1|\n",
      "|        Altinópolis|    1|\n",
      "|        Alto Alegre|    1|\n",
      "|           Alumínio|    1|\n",
      "|        Alvinlândia|    1|\n",
      "|          Americana|    1|\n",
      "|             Amparo|    1|\n",
      "|Américo Brasiliense|    1|\n",
      "|  Américo de Campos|    1|\n",
      "|          Analândia|    1|\n",
      "|          Andradina|    1|\n",
      "|           Angatuba|    1|\n",
      "|            Anhembi|    1|\n",
      "+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cases.select(['city']).groupby('city').count().sort('city', ascending=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We have one city without name. It's occur because the first information is about the whole state, so, we need to exclude this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_cases = df_cases.filter(df_cases.city.isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now, let's see if any numeric information is invalid (negative number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------+----------+------+--------------------+-------------------------+\n",
      "|confirmed|confirmed_per_100k_inhabitants|death_rate|deaths|estimated_population|estimated_population_2019|\n",
      "+---------+------------------------------+----------+------+--------------------+-------------------------+\n",
      "+---------+------------------------------+----------+------+--------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cases.select([\n",
    "    'confirmed',\n",
    "    'confirmed_per_100k_inhabitants',\n",
    "    'death_rate',\n",
    "    'deaths',\n",
    "    'estimated_population',\n",
    "    'estimated_population_2019'\n",
    "]).where(\n",
    "    (col('confirmed') < 0) |\n",
    "    (col('confirmed_per_100k_inhabitants') < 0) |\n",
    "    (col('death_rate') < 0) |\n",
    "    (col('deaths') < 0) | \n",
    "    (col('estimated_population') < 0) |\n",
    "    (col('estimated_population_2019') < 0)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "All numerical information looks right. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 3 - Define the Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 3.1 Conceptual Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "![Database Schema](./schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "In this project, we created 8 tables.\n",
    "\n",
    "- **city**: This table contains only the city information;\n",
    "- **vaccines**: This table stores the data about the vaccines;\n",
    "- **vaccination_group**: This table contains the data about vaccination groups that has priority;\n",
    "- **establishment**: This table contains de information about establishments thats applies the vaccine;\n",
    "- **vaccination**: This table stores the data about a person who has vaccinated;\n",
    "- **cases**: This table stores the data of accumulated cases and deaths for each day;\n",
    "- **hospital**: The hospital table stores the data about hospital overwhelmed;\n",
    "- **epidemic_day_fact**: This is a fact table and its responsible for concentrate all information for each day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 3.2 Mapping Out Data Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### City\n",
    "For this table, we're going to use the cases data as a reference, because in this dataset, there are all cities of São Paulo state. So, we need to extract the city name and city code and uses this as an input to fill this table. For the name of the city, we'll need to normalize the string, removing special characters and transforming the string to lower case.  \n",
    "\n",
    "#### Vaccination Group\n",
    "The vaccination group are present into the vaccination dataset. We only need get the groups ids (and cast this for int type) and names from this dataset, without duplication and removing the field without date vaccination, and fill the vaccination groups table.\n",
    "\n",
    "#### Vaccines\n",
    "For fill the vaccines table, we need to use the data from the vaccination dataset, selecting all differents vaccines for the code (and cast this for int type) and its respective names. It's necessary filter the data to select only lines with valid date vaccination.\n",
    "\n",
    "#### Establishment\n",
    "For fill this table, we need to use the data from the vaccination dataset, selecting all differents establishments for the code (and cast this for int type) and its respective names. It's necessary filter the data to select only lines with valid date vaccination.\n",
    "\n",
    "#### Vaccination\n",
    "This table is filled by the vaccination data. Before insert the data here, we must to cast the string for the datetime into date format, treat the age information, setting null for dates grather than 110 years and less than 18 years. We also must filter the data without vaccination date and generate an unique id.\n",
    "\n",
    "#### Hospital\n",
    "This table is originated by hospital overwhelmed data. To add data here, we must filter the data for São Paulo state and cast the datetime string for datetime type.\n",
    "\n",
    "#### Cases\n",
    "For this table, we only have to cast the date string to date type, exclude the line without city name and generate an unique id.\n",
    "\n",
    "#### Epidemic Day\n",
    "This table is responsible for aggregate all data from others tables for each day. We just define a primary key based on the date and include the foreign keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 4: Run Pipelines to Model the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.1 Create the data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### City Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /opt/conda/lib/python3.6/site-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_city_data(spark, cases_path, output_path):\n",
    "    \"\"\"\n",
    "    This function is responsable for proccess city_data files and save them into the output.\n",
    "    INPUTS: \n",
    "    * spark: spark session\n",
    "    * cases_path: cases dataset path\n",
    "    * output_path: path where process cities data will be save\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the dataset\n",
    "    cases_df = spark.read.json(cases_path)\n",
    "    \n",
    "    # filter lines without city name\n",
    "    cases_df = cases_df.filter(cases_df.city.isNotNull())\n",
    "    \n",
    "    # normalized strings\n",
    "    normalize_string = udf(lambda x: unidecode(x).lower(), StringType())\n",
    "    \n",
    "    city_table = cases_df.withColumn(\n",
    "        'city_normalized_name', normalize_string(col('city'))\n",
    "    ).withColumnRenamed(\n",
    "        'city_normalized_name', 'nome'\n",
    "    ).withColumnRenamed(\n",
    "        'city_ibge_code', 'id'\n",
    "    ).select([\n",
    "        'id', \n",
    "        'nome'\n",
    "    ]).dropDuplicates(['id'])\n",
    "\n",
    "    city_table.write.mode(\"overwrite\").parquet(os.path.join(output_path, 'cities'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "process_city_data(spark, './covid19_data/api_covid.json', './covid_19_results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Vaccination Group Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_vaccination_group_data(spark, vaccination_path, output_path):\n",
    "    \"\"\"\n",
    "    This function is responsable for proccess vaccination_group_data files \n",
    "    and save them into the output.\n",
    "    INPUTS: \n",
    "    * spark: spark session\n",
    "    * vaccination_path: vaccination dataset path\n",
    "    * output_path: path where process cities data will be save\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the dataset\n",
    "    vaccination_df = spark.read.option('delimiter', ';').option('header', 'true').csv(vaccination_path)\n",
    "    \n",
    "    # removing data without vaccination date\n",
    "    vaccination_df = vaccination_df.filter(vaccination_df.vacina_dataAplicacao.isNotNull())\n",
    "    \n",
    "    # cast ids to int type\n",
    "    cast_int = udf(lambda x: int(x), IntegerType())\n",
    "  \n",
    "    vaccination_table = vaccination_df.withColumn(\n",
    "        'grupoAtendimento_codigo_int', \n",
    "        cast_int(col('vacina_grupoAtendimento_codigo'))\n",
    "    ).withColumnRenamed(\n",
    "        'grupoAtendimento_codigo_int', 'id'\n",
    "    ).withColumnRenamed(\n",
    "        'vacina_grupoAtendimento_nome', 'nome'\n",
    "    ).select([\n",
    "        'id', \n",
    "        'nome'\n",
    "    ]).dropDuplicates(['id'])\n",
    "    \n",
    "    vaccination_table.write.mode(\"overwrite\").parquet(os.path.join(output_path, 'vaccination_groups'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "process_vaccination_group_data(spark, './covid19_data/vacinacao_SP.csv', './covid_19_results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Vaccines Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_vaccines_data(spark, vaccination_path, output_path):\n",
    "    \"\"\"\n",
    "    This function is responsable for proccess vaccines_data files \n",
    "    and save them into the output.\n",
    "    INPUTS: \n",
    "    * spark: spark session\n",
    "    * vaccination_path: vaccination dataset path\n",
    "    * output_path: path where process cities data will be save\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the dataset\n",
    "    vaccination_df = spark.read.option('delimiter', ';').option('header', 'true').csv(vaccination_path)\n",
    "    \n",
    "    # removing data without vaccination date\n",
    "    vaccination_df = vaccination_df.filter(vaccination_df.vacina_dataAplicacao.isNotNull())\n",
    "    \n",
    "    # cast ids to int type\n",
    "    cast_int = udf(lambda x: int(x) if x is not None else 0, IntegerType())\n",
    "  \n",
    "    vaccines_table = vaccination_df.withColumn(\n",
    "        'vacina_categoria_codigo_int', \n",
    "        cast_int(col('vacina_categoria_codigo'))\n",
    "    ).withColumnRenamed(\n",
    "        'vacina_categoria_codigo_int', 'id'\n",
    "    ).withColumnRenamed(\n",
    "        'vacina_grupoAtendimento_nome', 'nome'\n",
    "    ).withColumnRenamed(\n",
    "        'vacina_lote', 'lote'\n",
    "    ).withColumnRenamed(\n",
    "        'vacina_fabricante_nome', 'fabricante_nome'\n",
    "    ).withColumnRenamed(\n",
    "        'vacina_fabricante_referencia', 'fabricante_referencia'\n",
    "    ).select([\n",
    "        'id',\n",
    "        'nome',\n",
    "        'lote',\n",
    "        'fabricante_nome',\n",
    "        'fabricante_referencia'\n",
    "    ]).dropDuplicates(['id'])\n",
    "\n",
    "    vaccines_table.write.mode(\"overwrite\").parquet(os.path.join(output_path, 'vaccines'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "process_vaccines_data(spark, './covid19_data/vacinacao_SP.csv', './covid_19_results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Establishment Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_establishment_data(spark, vaccination_path, output_path):\n",
    "    \"\"\"\n",
    "    This function is responsable for proccess establishment_data files \n",
    "    and save them into the output.\n",
    "    INPUTS: \n",
    "    * spark: spark session\n",
    "    * vaccination_path: vaccination dataset path\n",
    "    * output_path: path where process cities data will be save\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the dataset\n",
    "    vaccination_df = spark.read.option('delimiter', ';').option('header', 'true').csv(vaccination_path)\n",
    "    \n",
    "    # removing data without vaccination date\n",
    "    vaccination_df = vaccination_df.filter(vaccination_df.vacina_dataAplicacao.isNotNull())\n",
    "    \n",
    "    # cast ids to int type\n",
    "    cast_int = udf(lambda x: int(x) if x is not None else 0, IntegerType())\n",
    "  \n",
    "    establishment_table = vaccination_df.withColumn(\n",
    "        'estabelecimento_valor_int', \n",
    "        cast_int(col('estabelecimento_valor'))\n",
    "    ).withColumn(\n",
    "        'estabelecimento_municipio_codigo_int',\n",
    "        cast_int(col('estabelecimento_municipio_codigo'))\n",
    "    ).withColumnRenamed(\n",
    "        'estabelecimento_valor_int', 'id'\n",
    "    ).withColumnRenamed(\n",
    "        'estabelecimento_razaoSocial', 'razaoSocial'\n",
    "    ).withColumnRenamed(\n",
    "        'estalecimento_noFantasia', 'nomeFantasia'\n",
    "    ).withColumnRenamed(\n",
    "        'estabelecimento_municipio_codigo_int', 'id_city'\n",
    "    ).select([\n",
    "        'id',\n",
    "        'razaoSocial',\n",
    "        'nomeFantasia',\n",
    "        'id_city',\n",
    "    ]).dropDuplicates(['id'])\n",
    "    \n",
    "    establishment_table.write.mode(\"overwrite\").parquet(os.path.join(output_path, 'establishment'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "process_establishment_data(spark, './covid19_data/vacinacao_SP.csv', './covid_19_results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Vaccination Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_vaccination_data(spark, vaccination_path, output_path):\n",
    "    \"\"\"\n",
    "    This function is responsable for proccess vaccination_data files \n",
    "    and save them into the output.\n",
    "    INPUTS: \n",
    "    * spark: spark session\n",
    "    * vaccinations_path: vaccination dataset path\n",
    "    * output_path: path where process cities data will be save\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the dataset\n",
    "    vaccination_df = spark.read.option('delimiter', ';').option('header', 'true').csv(vaccination_path)\n",
    "    \n",
    "    # removing data without vaccination date\n",
    "    vaccination_df = vaccination_df.filter(vaccination_df.vacina_dataAplicacao.isNotNull())\n",
    "    \n",
    "    # cast ids to int type\n",
    "    cast_int = udf(lambda x: int(x) if x is not None else 0, IntegerType())\n",
    "    get_date = udf(lambda x: datetime.strptime(x[0:10], '%Y-%m-%d'), DateType())\n",
    "    cast_age = udf(lambda x: int(x) if int(x) <= 110 or int(x) <= 18 else None, IntegerType())\n",
    "  \n",
    "    vaccines_table = vaccination_df.withColumn(\n",
    "        'paciente_idade_int', \n",
    "        cast_age(col('paciente_idade'))\n",
    "    ).drop(\n",
    "        'paciente_idade'\n",
    "    ).withColumn(\n",
    "        'vacina_dataAplicacao_date',\n",
    "        get_date(col('vacina_dataAplicacao'))\n",
    "    ).withColumnRenamed(\n",
    "        'paciente_idade_int', 'paciente_idade'\n",
    "    ).withColumnRenamed(\n",
    "        'vacina_dataAplicacao_date', 'vacina_data_aplicacao'\n",
    "    ).withColumn(\n",
    "        'vaccine_id',\n",
    "        cast_int(col('vacina_categoria_codigo'))\n",
    "    ).withColumn(\n",
    "        'id_vaccination_group',\n",
    "        cast_int(col('vacina_grupoAtendimento_codigo'))\n",
    "    ).withColumn(\n",
    "        'city_id_estabelecimento',\n",
    "        cast_int(col('estabelecimento_municipio_codigo'))\n",
    "    ).withColumn(\n",
    "        'city_id_paciente',\n",
    "        cast_int(col('paciente_endereco_coIbgeMunicipio'))\n",
    "    ).withColumn(\n",
    "        'id_establishment',\n",
    "        cast_int(col('estabelecimento_valor'))\n",
    "    ).withColumn(\n",
    "        'id', row_number().over(Window.orderBy(monotonically_increasing_id()))\n",
    "    ).select([\n",
    "        'id',\n",
    "        'paciente_id',\n",
    "        'paciente_idade',\n",
    "        'paciente_enumSexoBiologico',\n",
    "        'paciente_racaCor_valor',\n",
    "        'paciente_endereco_nmPais',\n",
    "        'paciente_endereco_uf',\n",
    "        'paciente_nacionalidade_enumNacionalidade',\n",
    "        'vacina_data_aplicacao',\n",
    "        'vacina_descricao_dose',\n",
    "        'vaccine_id',\n",
    "        'id_vaccination_group',\n",
    "        'city_id_estabelecimento',\n",
    "        'city_id_paciente',\n",
    "        'id_establishment'\n",
    "    ])\n",
    "    \n",
    "    vaccines_table.write.mode(\"overwrite\").partitionBy('vacina_data_aplicacao').parquet(os.path.join(output_path, 'vaccination'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "process_vaccination_data(spark, './covid19_data/vacinacao_SP.csv', './covid_19_results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Hospital Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_hospital_data(spark, hospital_path, cities_parquet_path, output_path):\n",
    "    \"\"\"\n",
    "    This function is responsable for proccess hospital_data files \n",
    "    and save them into the output.\n",
    "    INPUTS: \n",
    "    * spark: spark session\n",
    "    * hospital_path: hospital overwhelmed dataset path\n",
    "    * cities_parquet_path: path of parquet cities file\n",
    "    * output_path: path where process cities data will be save\n",
    "    \"\"\"\n",
    "       \n",
    "    # get city table\n",
    "    city_table = spark.read.parquet(cities_parquet_path)\n",
    "\n",
    "    # get hospital dataset\n",
    "    hospital_df = spark.read.option('delimiter', ';').option('header', 'true').csv(hospital_path)\n",
    "    \n",
    "    # get data from São Paulo state\n",
    "    hospital_df = hospital_df.filter(hospital_df.estado == 'São Paulo')\n",
    "    \n",
    "\n",
    "    get_date = udf(lambda x: datetime.strptime(x[0:10], '%Y-%m-%d'), DateType())\n",
    "    normalize_string = udf(lambda x: unidecode(x).lower(), StringType())\n",
    "    \n",
    "    hospital_df = hospital_df.withColumn(\n",
    "        'dataNotificacao_date', \n",
    "        get_date(col('dataNotificacao'))\n",
    "    )\n",
    "    hospital_df = hospital_df.filter(hospital_df.dataNotificacao_date > '2020-01-01')\n",
    "    \n",
    "    # normalize cities names to join city table\n",
    "    hospital_df = hospital_df.withColumn('city_name_join', normalize_string('municipio'))\n",
    "    \n",
    "    hospital_table = hospital_df.join(\n",
    "        city_table, (city_table.nome == hospital_df.city_name_join)\n",
    "    ).select(\n",
    "        col('_id').alias('id'),\n",
    "        col('dataNotificacao'),\n",
    "        col('ocupacaoSuspeitoCli'), \n",
    "        col('ocupacaoSuspeitoUti'), \n",
    "        col('ocupacaoConfirmadoCli'), \n",
    "        col('ocupacaoConfirmadoUti'), \n",
    "        col('saidaSuspeitaObitos'), \n",
    "        col('saidaSuspeitaAltas'), \n",
    "        col('saidaConfirmadaObitos'), \n",
    "        col('saidaConfirmadaAltas'),\n",
    "        city_table.id.alias('city_id')\n",
    "    ).dropDuplicates(['id'])\n",
    "    \n",
    "    hospital_table.write.mode(\"overwrite\").parquet(os.path.join(output_path, 'hospital'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "process_hospital_data(spark, './covid19_data/leitos_BR.csv', './covid_19_results/cities', './covid_19_results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cases Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_cases_group_data(spark, cases_path, output_path):\n",
    "    \"\"\"\n",
    "    This function is responsable for proccess hospital_data files \n",
    "    and save them into the output.\n",
    "    INPUTS: \n",
    "    * spark: spark session\n",
    "    * cases_path: cases dataset path\n",
    "    * output_path: path where process cities data will be save\n",
    "    \"\"\"\n",
    "    \n",
    "    # get cases dataset\n",
    "    cases_df = spark.read.json(cases_path)\n",
    "    \n",
    "    # filter lines without city name\n",
    "    cases_df = cases_df.filter(cases_df.city.isNotNull())\n",
    "    \n",
    "    # cast string to date\n",
    "    cast_date = udf(lambda x: datetime.strptime(x, '%Y-%m-%d'), DateType())\n",
    "    \n",
    "    cases_table = cases_df.withColumnRenamed(\n",
    "        'city_ibge_code', 'city_id'\n",
    "    ).withColumn(\n",
    "        'id', row_number().over(Window.orderBy(monotonically_increasing_id()))\n",
    "    ).withColumn(\n",
    "        'date_parsed', cast_date(col('date'))\n",
    "    ).drop(\n",
    "        'date'\n",
    "    ).withColumnRenamed(\n",
    "        'date_parsed',\n",
    "        'date'\n",
    "    ).select([\n",
    "        'id',\n",
    "        'confirmed',\n",
    "        'confirmed_per_100k_inhabitants',\n",
    "        'death_rate',\n",
    "        'deaths',\n",
    "        'estimated_population',\n",
    "        'estimated_population_2019',\n",
    "        'is_last',\n",
    "        'order_for_place',\n",
    "        'place_type',\n",
    "        'date',\n",
    "        'city_id'\n",
    "    ])\n",
    "    \n",
    "    cases_table.write.mode(\"overwrite\").partitionBy('date').parquet(os.path.join(output_path, 'cases'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "process_cases_group_data(spark, './covid19_data/api_covid.json', './covid_19_results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Epidemic Day Fact Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_epidemic_day_fact_data(spark, hospital_parquet_path, cases_parquet_path, vaccination_parquet_path, output_path):\n",
    "    \"\"\"\n",
    "    This function is responsable for proccess epidemic_day_fact_data files \n",
    "    and save them into the output.\n",
    "    INPUTS: \n",
    "    * spark: spark session\n",
    "    * hospital_parquet_path: hospital parquet path\n",
    "    * cases_parquet_path: cases parquet path\n",
    "    * vaccination_parquet_path: vaccination parquet path\n",
    "    * output_path: path where process cities data will be save\n",
    "    \"\"\"\n",
    "\n",
    "    cases_table = spark.read.parquet(\n",
    "        cases_parquet_path\n",
    "    ).withColumnRenamed(\n",
    "        'id','cases_id'\n",
    "    ).withColumnRenamed(\n",
    "        'city_id','city_ibge_code'\n",
    "    )\n",
    "    \n",
    "    hospital_table = spark.read.parquet(\n",
    "        hospital_parquet_path\n",
    "    ).withColumnRenamed(\n",
    "        'id','hospital_notificacao_id'\n",
    "    ).withColumnRenamed(\n",
    "        'city_id','hospital_city_id'\n",
    "    )\n",
    "    \n",
    "    vaccination_table = spark.read.parquet(\n",
    "        vaccination_parquet_path\n",
    "    ).withColumnRenamed(\n",
    "        'id','vaccination_id'\n",
    "    ).withColumnRenamed(\n",
    "        'city_id_estabelecimento','city_id_establishment'\n",
    "    )    \n",
    "        \n",
    "    epidemic_day_fact_table = hospital_table.join(\n",
    "        cases_table, \n",
    "        hospital_table.dataNotificacao == cases_table.date,\n",
    "        'full'\n",
    "    ).join(\n",
    "        vaccination_table,\n",
    "        hospital_table.dataNotificacao == vaccination_table.vacina_data_aplicacao,\n",
    "        'full'\n",
    "    ).selectExpr(\n",
    "        'IF( \\\n",
    "            date IS NOT NULL, \\\n",
    "            CONCAT(city_ibge_code, \"-\", date), \\\n",
    "            IF( \\\n",
    "                dataNotificacao IS NOT NULL, \\\n",
    "                CONCAT(hospital_city_id, \"-\", dataNotificacao), \\\n",
    "                IF( \\\n",
    "                    vacina_data_aplicacao IS NOT NULL, \\\n",
    "                    CONCAT(city_id_establishment, \"-\", vacina_data_aplicacao), \\\n",
    "                    \"\" \\\n",
    "                ) \\\n",
    "            ) \\\n",
    "        ) AS city_date',\n",
    "        'IF( \\\n",
    "            date IS NOT NULL, \\\n",
    "            date, \\\n",
    "            IF( \\\n",
    "                dataNotificacao IS NOT NULL, \\\n",
    "                dataNotificacao, \\\n",
    "                IF( \\\n",
    "                    vacina_data_aplicacao IS NOT NULL, \\\n",
    "                    vacina_data_aplicacao, \\\n",
    "                    \"\" \\\n",
    "                ) \\\n",
    "            ) \\\n",
    "        ) AS date',\n",
    "        'hospital_notificacao_id',\n",
    "        'cases_id', \n",
    "        'vaccination_id',\n",
    "    )\n",
    "    \n",
    "    epidemic_day_fact_table.write.mode(\"overwrite\").parquet(os.path.join(output_path, 'epidemic_day_fact'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "process_epidemic_day_fact_data(spark, './covid_19_results/hospital', './covid_19_results/cases', './covid_19_results/vaccination', './covid_19_results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.2 Data Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### City Table\n",
    "For this table, we'll verify the quantity of cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_cities_parquet = spark.read.parquet(\"./covid_19_results/cities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "assert df_cities_parquet.count() == 646, \"Should be 646\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Vaccination Group Table\n",
    "For this table, we'll verify the quantity of groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_vaccination_group_parquet = spark.read.parquet(\"./covid_19_results/vaccination_groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "assert df_vaccination_group_parquet.count() == 36, \"Should be 36\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Vaccines Table\n",
    "For this table, we'll verify the quantity of vaccines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_vaccines_parquet = spark.read.parquet(\"./covid_19_results/vaccines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "assert df_vaccines_parquet.count() == 9, \"Should be 9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Establishment Table\n",
    "For this table, we'll count the quantity of establishments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_establishment_parquet = spark.read.parquet(\"./covid_19_results/establishment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "assert df_establishment_parquet.count() == 4355, \"Should be 4355\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Vaccination Table\n",
    "For this table, we'll check 4 things\n",
    "- Quantity of establishments\n",
    "- Quantity of vaccination groups\n",
    "- Quantity of different vaccines \n",
    "- Quantity of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_vaccination_parquet = spark.read.parquet(\"./covid_19_results/vaccination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "assert df_vaccination_parquet.select(['id_establishment']).dropDuplicates(['id_establishment']).count() == 4355, \"Should be 4355\"\n",
    "assert df_vaccination_parquet.select(['id_vaccination_group']).dropDuplicates(['id_vaccination_group']).count() == 36, \"Should be 36\"\n",
    "assert df_vaccination_parquet.select(['vaccine_id']).dropDuplicates(['vaccine_id']).count() == 9, \"Should be 9\"\n",
    "assert df_vaccination_parquet.count() == 958897, \"Should be 958897\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Hospital Table\n",
    "For this table, we'll check the quantity of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_hospital_parquet = spark.read.parquet(\"./covid_19_results/hospital\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "assert df_hospital_parquet.count() == 104845, \"Should be 104845\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cases Table\n",
    "For this table we'll check the quantity of line and the quantity of cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_cases_parquet = spark.read.parquet(\"./covid_19_results/cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "assert df_cases_parquet.dropDuplicates(['city_id']).count() == 646, \"Should be 646\"\n",
    "assert df_cases_parquet.count() == 646, \"Should be 646\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Epidemic Day Fact Table\n",
    "For this table we'll check 3 things:\n",
    "- Quantity of vaccination_pacients\n",
    "- Quantity of cases\n",
    "- Quantity of hospital notification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_epidemic_day_fact_parquet = spark.read.parquet(\"./covid_19_results/epidemic_day_fact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "assert df_epidemic_day_fact_parquet.dropDuplicates(['cases_id']).where(col('cases_id').isNotNull()).count() == 646, \"Should be 646\"\n",
    "assert df_epidemic_day_fact_parquet.dropDuplicates(['hospital_notificacao_id']).where(col('hospital_notificacao_id').isNotNull()).count() == 104845, \"Should be 104845\"\n",
    "assert df_epidemic_day_fact_parquet.dropDuplicates(['vaccination_id']).where(col('vaccination_id').isNotNull()).count() == 958897, \"Should be 958897\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.3 Data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### City Table\n",
    "\n",
    "\n",
    "| Field | Type | Description |\n",
    "| ----- |------|-------------|\n",
    "| id | int (PK) | IBGE city code |\n",
    "| nome | varchar(255) | Name of city |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Establishment Table\n",
    "\n",
    "\n",
    "| Field | Type | Description |\n",
    "| ----- |------|-------------|\n",
    "| id | int (PK) | Establishment ID |\n",
    "| razaoSocial | varchar(255) | Name of the company |\n",
    "| nomeFantasia | varchar(255) | Name of establishment |\n",
    "| id_city | int (FK) | IBGE code of city |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Vaccination Group Table\n",
    "\n",
    "\n",
    "| Field | Type | Description |\n",
    "| ----- |------|-------------|\n",
    "| id | int (PK) | Vaccination Group ID |\n",
    "| nome | varchar(255) | Name of group |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Vaccines Table\n",
    "\n",
    "\n",
    "| Field | Type | Description |\n",
    "| ----- |------|-------------|\n",
    "| id | int (PK) | Vaccine ID |\n",
    "| lote | varchar(255) | Name of the production batch |\n",
    "| fabricante_nome | varchar(255) | name of fabricant |\n",
    "| fabricante_referencia | varchar(255) | Fabricant references |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Vaccination Table\n",
    "\n",
    "\n",
    "| Field | Type | Description |\n",
    "| ----- |------|-------------|\n",
    "| id | int (PK) | Vaccination ID |\n",
    "| paciente_id | varchar(255) | Pacient ID |\n",
    "| paciente_idade | int | Pacient age |\n",
    "| paciente_enumSexoBiologico | varchar(255) | Pacient gender |\n",
    "| paciente_racaCor_valor | varchar(255) | Pacient race |\n",
    "| paciente_endereco_nmPais | varchar(255) | Pacient country |\n",
    "| paciente_endereco_uf | varchar(255) | Pacient state |\n",
    "| paciente_nacionalidade_enumNacionalidade | varchar(255) | Pacient nationality |\n",
    "| vacina_data_aplicacao | date | Date of vaccine application |\n",
    "| descricao_dose | varchar(255) | Vaccine dose description |\n",
    "| vaccine_id | int (FK) | Vaccine ID |\n",
    "| id_vaccination_group | int (FK) | Vaccination group ID |\n",
    "| city_id_estabelecimento | int (FK) | IBGE code of establishment city |\n",
    "| city_id_paciente | int (FK) | IBGE code of pacient city |\n",
    "| id_establishment | int (FK) | Establishment ID |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Hospital Table\n",
    "\n",
    "\n",
    "| Field | Type | Description |\n",
    "| ----- |------|-------------|\n",
    "| id | int (PK) | Notification ID |\n",
    "| dataNotificacao | date | Notification date |\n",
    "| ocupacaoSuspeitoCli | int | Quantity of suspicious occupation CLI |\n",
    "| ocupacaoSuspeitoUti | int | Quantity of suspected ICU occupation |\n",
    "| ocupacaoConfirmadoCli | int | Quantity of occupation confirmed CLI |\n",
    "| ocupacaoConfirmadoUti | int | Quantity of confirmed ICU occupation |\n",
    "| saidaSuspeitaObitos | int | Quantity of suspect deaths |\n",
    "| saidaSuspeitaAltas | int | Quantity of recuperations suspect |\n",
    "| saidaConfirmadaObitos | int | Quantity of confirmed deaths |\n",
    "| saidaConfirmadaAltas | int | Quantity of recuperations confirmed |\n",
    "| city_id | int (FK) | IBGE city code |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cases Table\n",
    "\n",
    "\n",
    "| Field | Type | Description |\n",
    "| ----- |------|-------------|\n",
    "| id | int (PK) | Notification ID |\n",
    "| confirmed | int | Confirmed cases until the date |\n",
    "| confirmed_per_100k_inhabitants | int | Confirmed cases per 100k inhabitants until the date |\n",
    "| death_rate | float | Death rate until the date |\n",
    "| deaths | int | Total numbers of deaths until the date |\n",
    "| estimated_population | int | Estimated population |\n",
    "| estimated_population_2019 | int | Estimated population at 2019 |\n",
    "| is_last | boolean | This data is an update? |\n",
    "| order_for_place | int |Sorting for this location |\n",
    "| place_type | int | Data from state or city |\n",
    "| date | int | Date of notification |\n",
    "| cty_id | int (FK) | IBGE code of city |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Epidemic Day Fact Table\n",
    "\n",
    "\n",
    "| Field | Type | Description |\n",
    "| ----- |------|-------------|\n",
    "| city_date | int (PK) | Concat of IBGE code city and date |\n",
    "| date | date | date |\n",
    "| hospital_notificacao_id | int (FK) | Notification hospital ID |\n",
    "| cases_id | int (FK) | case ID |\n",
    "| vaccination_paciente_id | int (FK) | Vaccination Paciente ID |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "To process the data in this project, we used PySpark. The Spark was can be faster than Hadoop for large scale data processing, and we can process the data in a parallel way, so, it's easier to scale our cluster if it's necessary.\n",
    "All data are saved into parquet files. Parquet format was chosen because it's a format that can be stored into S3 and can be copied to the Redshift cluster with the same structure. So, it's easy to get data from parquet files, at a staging bucket and load this into a table at Redshift Cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Propose how often the data should be updated and why.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The data should be updated once a day. We are working with COVID 19 data, so, it's necessary at the end of the day for all the public and private organizations can send their notifications to the government."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Write a description of how you would approach the problem differently under the following scenarios:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    " * The data was increased by 100x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "If the data was increased by 100x we can expand our cluster nodes to process the data faster, it's an easy solution because we're using spark, so, updates at code aren't necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "If the data must update a dashboard at 7 a.m., we can set the ETL process to be executed at night. The data are available only at night when the notifications of the day are consolidated, so, it's not a problem to run the ETL process during the night. We also can create a new table with fields that are used by the dashboard and make this more performative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The tables are designed by the analytical team to have insights about the data, if the number of people that consult the database increase, we can create new OLAP cubes to make available the data faster for analytical queries. If another team must have access to the raw data, it's possible to create a staging step, that saves raw data into a bucket, so, the team can be accessed to the raw data at the bucket, the parquet files, and the tables at Redshift, depending on their necessities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
